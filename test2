import spacy
import re
from datetime import datetime, timedelta
from dateparser import parse as date_parse
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import FunctionTransformer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer

# --- 1. Load spaCy model ---
nlp = spacy.load("en_core_web_sm")

# --- 2. Synthetic Data (Replace with your actual labeled data) ---
# In a real scenario, you'd load this from a CSV, JSON, or database
# Note: 'action_date' is for verification during extraction, not directly for ML classification
# The ML model will classify 'is_action_item' based on other features.
# The date will be extracted post-classification.

current_date = datetime(2025, 6, 12) # For relative date parsing

labeled_data = [
    {"text": "Please finalize the report by next Tuesday.", "is_action_item": 1, "action_date": "2025-06-17"},
    {"text": "Hope you are doing well.", "is_action_item": 0, "action_date": None},
    {"text": "- Draft the presentation for Friday's meeting.", "is_action_item": 1, "action_date": "2025-06-13"},
    {"text": "I will send the meeting minutes by EOD.", "is_action_item": 1, "action_date": "2025-06-12"},
    {"text": "Meeting scheduled for 06/20/2025.", "is_action_item": 0, "action_date": "2025-06-20"}, # Not an action, just an info
    {"text": "Confirm attendance for the workshop on July 5.", "is_action_item": 1, "action_date": "2025-07-05"},
    {"text": "This is a general update regarding the project.", "is_action_item": 0, "action_date": None},
    {"text": "We need to follow up on the client's request.", "is_action_item": 1, "action_date": None}, # No specific date mentioned in sentence
    {"text": "Let's review the budget proposal ASAP.", "is_action_item": 1, "action_date": None},
    {"text": "Could you please review the attached document?", "is_action_item": 1, "action_date": None},
    {"text": "Attached is the Q2 financial report.", "is_action_item": 0, "action_date": None},
    {"text": "Ensure all documents are signed by Monday.", "is_action_item": 1, "action_date": "2025-06-16"},
    {"text": "The project will be completed by the end of next month.", "is_action_item": 0, "action_date": "2025-07-31"}, # Passive voice, less likely an direct action item for reader
    {"text": "Thanks for your help!", "is_action_item": 0, "action_date": None},
    {"text": "Please provide feedback on the draft.", "is_action_item": 1, "action_date": None},
]

df = pd.DataFrame(labeled_data)

# --- 3. Feature Engineering Classes ---

class TextFeatures(BaseEstimator, TransformerMixin):
    def __init__(self, nlp_model):
        self.nlp = nlp_model

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        features = []
        for text in X:
            doc = self.nlp(text)
            
            # Initialize feature dictionary
            feats = {
                'has_bullet_point': int(re.match(r'^\s*[-*â€¢]\s*', text) is not None),
                'starts_with_number_list': int(re.match(r'^\s*\d+\.\s*', text) is not None),
                'has_imperative_verb_at_start': 0,
                'has_should_must': int('should' in text.lower() or 'must' in text.lower()),
                'has_verb': int(any(token.pos_ == 'VERB' for token in doc)),
                'has_dobj': int(any(token.dep_ == 'dobj' for token in doc)),
                'has_aux_modal': int(any(token.dep_ == 'aux' and token.tag_ == 'MD' for token in doc)), # e.g., 'will', 'can'
                'line_length': len(text),
                'ends_with_period': int(text.strip().endswith('.')),
                'ends_with_question': int(text.strip().endswith('?'))
            }

            # Check for imperative verb at start (more sophisticated way)
            if doc:
                # Look for verbs that are typically imperative when at the start of a sentence
                # VB: Verb, base form (e.g., go, give)
                # VBP: Verb, non-3rd person singular present (e.g., eat, send)
                # If the first non-punctuation, non-space token is a verb and not passive
                first_meaningful_token = next((token for token in doc if not token.is_space and not token.is_punct), None)
                if first_meaningful_token and first_meaningful_token.pos_ == 'VERB':
                    # Avoid passive voice if possible, but simple check for now
                    if not any(child.dep_ == 'auxpass' for child in first_meaningful_token.children):
                         feats['has_imperative_verb_at_start'] = 1

            features.append(feats)
        return pd.DataFrame(features)

class DatePresenceTransformer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        has_date_list = []
        for text in X:
            # A simple check for date presence. Full parsing happens later.
            try:
                parsed_date = date_parse(text, settings={'RELATIVE_BASE': current_date})
                has_date_list.append(int(parsed_date is not None))
            except:
                has_date_list.append(0)
        return pd.DataFrame({'has_date': has_date_list})


# --- 4. Define Preprocessing and ML Pipeline ---

# We need to handle text features and potentially other numerical features.
# For text classification, we can use TF-IDF or just rely on the engineered features.
# Let's combine custom features with a simple TF-IDF on the raw text to capture more nuance.

# Features from TextFeatures class
text_feat_extractor = Pipeline([
    ('text_feats', TextFeatures(nlp)),
])

# Feature for date presence
date_presence_extractor = Pipeline([
    ('date_presence', DatePresenceTransformer()),
])

# Combine all features before classification
feature_union = FeatureUnion([
    ('nlp_features', text_feat_extractor),
    ('date_features', date_presence_extractor),
    # TF-IDF on raw text - this captures word importance without specific keywords
    ('tfidf', TfidfVectorizer(max_features=500, stop_words='english',
                              ngram_range=(1,2))) # Use ngrams for phrases
])


pipeline = Pipeline([
    ('features', feature_union),
    ('classifier', LogisticRegression(random_state=42, solver='liblinear')) # Robust for smaller datasets
])


# --- 5. Train the Model ---

X = df['text']
y = df['is_action_item']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("Training model...")
pipeline.fit(X_train, y_train)
print(f"Model trained. Accuracy on test set: {pipeline.score(X_test, y_test):.2f}")

# --- 6. Function to Extract Dates from a Text Line (Post-Classification) ---

def extract_action_date(text_line):
    """
    Attempts to parse a date from a given text line.
    Uses dateparser for robustness, with relative base date.
    """
    try:
        # Use dateparser for robust date extraction, considering "next Tuesday", "EOD", "tomorrow", etc.
        # Set a current date for accurate relative date parsing
        parsed_date = date_parse(text_line, settings={
            'RELATIVE_BASE': current_date, # Use the fixed current_date for consistency
            'RETURN_AS_TIMEZONE_AWARE': False, # Return naive datetime objects
            'REQUIRE_PARTS': ['day', 'month'] # Require at least day and month
        })
        if parsed_date:
            # Check if the parsed date is significantly in the past (e.g., more than a week)
            # This helps to filter out historical dates in the context of action items
            if (current_date - parsed_date).days > 7:
                 return None # Likely a historical date, not an action date
            return parsed_date.strftime('%Y-%m-%d')
    except Exception as e:
        # print(f"Error parsing date '{text_line}': {e}")
        pass
    return None

# --- 7. Apply to New Email Text ---

def process_email_for_action_items(email_content):
    lines = [line.strip() for line in email_content.split('\n') if line.strip()]
    action_items_found = []

    # Predict which lines are action items
    predictions = pipeline.predict(lines)

    for i, line in enumerate(lines):
        if predictions[i] == 1: # If the model predicts it's an action item
            # Try to extract date from the line itself
            action_date = extract_action_date(line)

            # If no date found in the line, check adjacent lines
            if action_date is None:
                # Check next line
                if i + 1 < len(lines):
                    action_date = extract_action_date(lines[i+1])
                # Check previous line (optional, might be less common for action dates)
                # if action_date is None and i - 1 >= 0:
                #     action_date = extract_action_date(lines[i-1])

            action_items_found.append({
                'action_item': line,
                'action_date': action_date
            })
    return action_items_found

# --- Example Usage ---
test_email = """
Hi Team,

Just a quick update:
- Project Alpha kick-off meeting next Wednesday.
- Finalize the budget report. Due by Friday.
- Please confirm your availability for the workshop on June 25, 2025.
- We should aim to complete the client presentation by July 10.
This is a general announcement.
I'm looking forward to hearing from you.

Regards,
Alice
"""

print("\n--- Processing Test Email ---")
identified_actions = process_email_for_action_items(test_email)

if identified_actions:
    for item in identified_actions:
        print(f"Action: '{item['action_item']}' | Date: {item['action_date']}")
else:
    print("No action items identified.")

