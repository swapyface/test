import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

# Sample Data (Replace with your actual dataset)
data = {
    'employee_id': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
    'feature1': [23, 45, 12, 67, 34, 89, 21, 56, 78, 43],
    'feature2': [5.6, 7.8, 6.5, 8.9, 5.4, 9.1, 6.2, 7.5, 8.3, 5.9],
    'feature3': [100, 200, 150, 300, 120, 330, 160, 250, 280, 130],
    'target': [5000, 7000, 4500, 9000, 5200, 9500, 4800, 7600, 8500, 5100]
}

# Create DataFrame
df = pd.DataFrame(data)

# Define features (excluding ID) and target
X = df[['feature1', 'feature2', 'feature3']]
y = df['target']
employee_ids = df['employee_id']  # Save Employee IDs

# Train-Test Split (80% Train, 20% Test)
X_train, X_test, y_train, y_test, id_train, id_test = train_test_split(
    X, y, employee_ids, test_size=0.2, random_state=42
)

# Standardize Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Linear Regression Model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Predictions
y_pred = model.predict(X_test_scaled)

# Print Unique ID with Actual and Predicted Values
result_df = pd.DataFrame({'employee_id': id_test, 'y_actual': y_test, 'y_pred': y_pred})
print(result_df)


import pandas as pd

# Sample data
df = pd.DataFrame({'up-date': ['31-mar-2025']})

# Convert to datetime format
df['up-date'] = pd.to_datetime(df['up-date'], format='%d-%b-%Y')

# Change to 'YYYY-MM-DD' format
df['up-date'] = df['up-date'].dt.strftime('%Y-%m-%d')

print(df)

from sklearn.neighbors import NearestNeighbors
import numpy as np
import matplotlib.pyplot as plt

k = 5  # Set k = min_samples (start with 5)
nbrs = NearestNeighbors(n_neighbors=k).fit(X_scaled)
distances, indices = nbrs.kneighbors(X_scaled)

# Sort and plot the k-th nearest neighbor distances
distances = np.sort(distances[:, k-1])  # k-th distance for each point
plt.plot(distances)
plt.xlabel("Points sorted by distance")
plt.ylabel(f"{k}-th Nearest Neighbor Distance")
plt.title("Elbow Method for Choosing eps")
plt.show()
import numpy as np
from sklearn.cluster import DBSCAN

def get_eps(values, scale=1.5):
    """Compute adaptive eps using MAD (Median Absolute Deviation)."""
    median = np.median(values)
    mad = np.median(np.abs(values - median))
    return mad * scale  # Adjust scale based on tolerance

eps_dict = {}  # Store per-issuer eps values
for issuer, group in df.groupby("issuer"):  # Replace "issuer" with your column
    eps_dict[issuer] = get_eps(group["weighted_rating"])  # Replace with relevant column

# Apply DBSCAN separately for each issuer
df["dbscan_label"] = -1  # Default to outlier
for issuer, group in df.groupby("issuer"):
    eps_value = eps_dict[issuer]
    dbscan = DBSCAN(eps=eps_value, min_samples=2)  # Keep min_samples low
    labels = dbscan.fit_predict(group[["weighted_rating"]])  # Ensure proper feature selection
    df.loc[group.index, "dbscan_label"] = labels

2----
import scipy.stats as stats

def get_threshold(values, percentile=99):
    """Compute outlier threshold using percentiles."""
    return np.percentile(values, percentile)

thresholds = {issuer: get_threshold(group["weighted_rating"]) for issuer, group in df.groupby("issuer")}

df["is_outlier"] = False
for issuer, group in df.groupby("issuer"):
    threshold = thresholds[issuer]
    df.loc[group.index, "is_outlier"] = group["weighted_rating"] > threshold  # Mark only extreme cases

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Fit K-Means
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(data)

# Compute Silhouette Score
silhouette_avg = silhouette_score(data, labels)
print(f"Silhouette Score: {silhouette_avg}")

def get_eps(values, scale=1.5, min_eps=0.1):  
    median = np.median(values)
    mad = np.median(np.abs(values - median))
    std_dev = np.std(values)  # Standard deviation as a backup
    return max(mad * scale, std_dev * 0.5, min_eps)  # Choose the largest reasonable value

