import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder

class Preprocessor:
    def __init__(self, file_path, required_columns):
        """
        Initialize the Preprocessor class with file path and required columns.

        Parameters:
        - file_path: Path to the CSV file.
        - required_columns: List of columns required by the model for validation.
        """
        self.file_path = file_path
        self.required_columns = required_columns

    def load_data(self):
        try:
            data = pd.read_csv(self.file_path)
            print(f"File '{self.file_path}' loaded successfully.")
            return data
        except FileNotFoundError:
            raise FileNotFoundError(f"File '{self.file_path}' not found.")
        except pd.errors.EmptyDataError:
            raise ValueError(f"File '{self.file_path}' is empty.")
        except Exception as e:
            raise Exception(f"An error occurred while loading the file: {e}")

    def validate_columns(self, data):
        """
        Validate that the dataset contains the required columns for the model.
        """
        missing_columns = [col for col in self.required_columns if col not in data.columns]
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")
        print("All required columns are present.")

    def handle_missing_values(self, data):
        """
        Fill missing values based on data type:
        - Numeric columns: Fill with 0.
        - Object (categorical) columns: Fill with 'unknown'.
        - Date columns: Fill with a default value '1900-01-01'.
        """
        for col in data.columns:
            if pd.api.types.is_numeric_dtype(data[col]):
                data[col].fillna(0, inplace=True)
            elif pd.api.types.is_object_dtype(data[col]):
                data[col].fillna("unknown", inplace=True)
            elif pd.api.types.is_datetime64_any_dtype(data[col]):
                data[col].fillna(pd.to_datetime("1900-01-01", errors='coerce'), inplace=True)
        return data

    def preprocess(self):
        """
        Execute the preprocessing steps: load data, validate columns, handle missing values,
        standardize numerical columns, and one-hot encode categorical columns.
        """
        # Load the dataset
        data = self.load_data()

        # Validate the required columns
        self.validate_columns(data)

        # Handle missing values
        data = self.handle_missing_values(data)

        # Standardize numerical columns (e.g., 'price')
        scaler = StandardScaler()
        if 'price' in data.columns:
            data['scaled_price'] = scaler.fit_transform(data[['price']])

        # One-hot encode categorical columns (e.g., 'rating')
        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
        if 'rating' in data.columns:
            rating_encoded = encoder.fit_transform(data[['rating']])
            rating_encoded_df = pd.DataFrame(rating_encoded, columns=encoder.get_feature_names_out(['rating']))
            data = pd.concat([data, rating_encoded_df], axis=1)

        # Flag records with WR, NR, or unknown ratings
        flagged_data = data[data['rating'].isin(['WR', 'NR', 'unknown'])]
        non_flagged_data = data[~data['rating'].isin(['WR', 'NR', 'unknown'])]

        return flagged_data, non_flagged_data