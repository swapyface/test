import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Lambda
from tensorflow.keras.models import Model
from tensorflow.keras import backend as K
import numpy as np

# Set the latent space size
latent_dim = 4  # Adjust based on data complexity
input_dim = df.shape[1]  # Number of input features

# Define Encoder
inputs = Input(shape=(input_dim,))
h = Dense(16, activation='relu')(inputs)
h = Dense(8, activation='relu')(h)

# Define mean and variance for latent space
z_mean = Dense(latent_dim)(h)
z_log_var = Dense(latent_dim)(h)

# Sampling function
def sampling(args):
    z_mean, z_log_var = args
    batch = K.shape(z_mean)[0]
    dim = K.int_shape(z_mean)[1]
    epsilon = K.random_normal(shape=(batch, dim))
    return z_mean + K.exp(0.5 * z_log_var) * epsilon

z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])

# Define Decoder
decoder_h = Dense(8, activation='relu')
decoder_h2 = Dense(16, activation='relu')
decoder_mean = Dense(input_dim, activation='sigmoid')

h_decoded = decoder_h(z)
h_decoded = decoder_h2(h_decoded)
outputs = decoder_mean(h_decoded)

# Build VAE model
vae = Model(inputs, outputs)

# Define Loss Function (Reconstruction Loss + KL Divergence)
reconstruction_loss = tf.keras.losses.mean_squared_error(inputs, outputs)
reconstruction_loss = K.sum(reconstruction_loss)

kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)

vae_loss = K.mean(reconstruction_loss + kl_loss)
vae.add_loss(vae_loss)

# Compile and Train the VAE
vae.compile(optimizer='adam')
vae.fit(df, df, epochs=50, batch_size=32, shuffle=True, validation_split=0.1)

# Save the trained model
vae.save("vae_model.h5")

def detect_outliers_vae(new_data, vae, w2v_model, encoders, threshold=0.05):
    """Detect outliers using VAE-based anomaly detection, returning results with unique identifiers."""
    
    unique_id = new_data.pop("Unique_ID")  # Extract unique identifier
    
    # Convert company name to vector
    company_vector = get_company_vector(new_data['Company'], w2v_model)

    # Convert dictionary to DataFrame
    new_df = pd.DataFrame([new_data])

    # Encode categorical variables
    for col in ['Rating', 'Group', 'Industry']:
        if col in new_df.columns and col in encoders:
            try:
                new_df[col] = encoders[col].transform(new_df[col])
            except ValueError:
                return {"Unique_ID": unique_id, "Prediction": "Outlier (Unknown Category)"}

    # Drop original 'Company' column and add vector representation
    new_df = new_df.drop(columns=['Company'])
    new_df = pd.concat([new_df, pd.DataFrame([company_vector])], axis=1)

    # Get VAE reconstructed output
    reconstructed = vae.predict(new_df)
    
    # Compute reconstruction error
    error = np.mean(np.abs(new_df.values - reconstructed), axis=1)

    # Flag as outlier if error exceeds threshold
    result = "Outlier" if error[0] > threshold else "Normal"

    return {"Unique_ID": unique_id, "Prediction": result}


from tensorflow.keras.models import load_model
import joblib

# Load trained models
vae = load_model("vae_model.h5")
w2v_model = Word2Vec.load("company_w2v.model")
encoders = joblib.load("encoders.pkl")

# Example new data
new_record = {
    "Company": "Apple",
    "Rating": "D",  # Unusual rating
    "Group": "Tech",
    "Industry": "Software"
}

result = detect_outliers_vae(new_record, vae, w2v_model, encoders, threshold=0.05)
print(f"Prediction: {result}")  # Likely "Outlier"


# Example new data with Unique ID
new_record = {
    "Unique_ID": 12345,
    "Company": "Apple",
    "Rating": "D",  # Unusual rating
    "Group": "Tech",
    "Industry": "Software"
}

# Detect outlier
result = detect_outliers_vae(new_record, vae, w2v_model, encoders, threshold=0.05)
print(result)


