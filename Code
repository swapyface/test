import pandas as pd
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load data in chunks to reduce memory usage
chunk_size = 100000
chunk_iterator = pd.read_csv('your_large_dataset.csv', chunksize=chunk_size)

# Output file for storing intermediate results
output_file = 'dbscan_outliers_with_flagged_ratings.csv'

# Initialize a flag for writing header to CSV
write_header = True

# Process each chunk separately
for chunk_index, chunk in enumerate(chunk_iterator):
    # Handle missing values
    chunk['price'].fillna(0, inplace=True)
    chunk['rating'].fillna('unknown', inplace=True)

    # Flag specific ratings as outliers
    chunk['Is_outlier'] = chunk['rating'].isin(['WR', 'NR', 'unknown'])

    # Separate flagged data from the rest
    flagged_outliers = chunk[chunk['Is_outlier']].copy()
    remaining_data = chunk[~chunk['Is_outlier']].copy()

    # Process the remaining data by rating
    unique_ratings = remaining_data['rating'].unique()

    for rating in unique_ratings:
        # Filter data for the current rating
        rating_data = remaining_data[remaining_data['rating'] == rating].copy()

        # Scale numerical columns (price)
        scaler = StandardScaler()
        rating_data['scaled_price'] = scaler.fit_transform(rating_data[['price']])

        # Set DBSCAN parameters based on rating
        if rating == 'High':
            eps = 0.8
            min_samples = 5
        elif rating == 'Medium':
            eps = 0.5
            min_samples = 4
        else:  # For 'Low'
            eps = 0.3
            min_samples = 3

        # Apply DBSCAN
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        rating_data['label'] = dbscan.fit_predict(rating_data[['scaled_price']])

        # Calculate Silhouette Score if more than one cluster is found
        unique_labels = set(rating_data['label'])
        if len(unique_labels) > 1 and -1 not in unique_labels:
            score = silhouette_score(rating_data[['scaled_price']], rating_data['label'])
            print(f"Chunk: {chunk_index + 1}, Rating: {rating}, Silhouette Score: {score:.2f}")
        else:
            print(f"Chunk: {chunk_index + 1}, Rating: {rating}, Silhouette Score cannot be calculated due to insufficient clusters.")

        # Mark labels as "outlier" or "inlier"
        rating_data['label'] = rating_data['label'].apply(lambda x: 'outlier' if x == -1 else 'inlier')

        # Append flagged outliers for this rating
        flagged_outliers = pd.concat([flagged_outliers, rating_data])

    # Save intermediate results to CSV
    flagged_outliers.to_csv(output_file, mode='a', header=write_header, index=False)
    write_header = False  # Only write header for the first chunk

print("Processing completed and results saved to:", output_file)

# (Optional) Visualization step for the final data
final_data = pd.read_csv(output_file)

plt.figure(figsize=(10, 6))
sns.boxplot(y=final_data['price'], showfliers=False)

# Plot outliers in red and inliers in blue
outliers = final_data[final_data['label'] == 'outlier']
inliers = final_data[final_data['label'] == 'inlier']

# Plot individual points
plt.scatter(np.full(inliers.shape[0], 0), inliers['price'], color='blue', label='Inlier', edgecolor='black')
plt.scatter(np.full(outliers.shape[0], 0), outliers['price'], color='red', label='Outlier', edgecolor='black')

# Customize plot
plt.legend(loc='upper right')
plt.title('DBSCAN Outlier Detection with Flagged Ratings')
plt.ylabel('Price')
plt.show()
