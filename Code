from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
import numpy as np


# Function to encode data
def encoded_data(rating_data, numerical_col, categorical_col):
    # Scale numerical data
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(rating_data[numerical_col])
    
    # Encode categorical data
    encoder = OneHotEncoder(sparse=False, drop='first')
    encoded_categorical = encoder.fit_transform(rating_data[categorical_col])
    
    # Combine encoded data
    X = np.hstack((encoded_categorical, scaled_data))
    return X


# Function to determine parameters based on dataset size
def get_iso_params(dataset_size):
    if dataset_size < 1000:
        return {"n_estimators": 3, "max_samples": 2, "contamination": 0.1}
    elif 1000 <= dataset_size < 10000:
        return {"n_estimators": 50, "max_samples": 64, "contamination": 0.05}
    elif 10000 <= dataset_size < 50000:
        return {"n_estimators": 100, "max_samples": 128, "contamination": 0.02}
    else:
        return {"n_estimators": 200, "max_samples": 256, "contamination": 0.01}


# Function to process each rating
def process_rating(rating, data, numerical_col, categorical_col):
    try:
        print(f"Processing Rating: {rating}")
        
        # Filter data for the specific rating
        rating_data = data[data['Rating'] == rating]
        
        # Determine Isolation Forest parameters
        params = get_iso_params(len(rating_data))
        
        # Initialize Isolation Forest model
        iso_model = IsolationForest(
            n_estimators=params["n_estimators"],
            max_samples=params["max_samples"],
            contamination=params["contamination"],
            random_state=42
        )
        
        # Encode data
        X = encoded_data(rating_data, numerical_col, categorical_col)
        
        # Predict outliers
        rating_data['IsOutlier'] = iso_model.fit_predict(X)
        
        # Replace -1 and 1 with "outlier" and "inlier"
        rating_data['IsOutlier'] = rating_data['IsOutlier'].replace({-1: "outlier", 1: "inlier"})
        
        return rating_data
    except Exception as e:
        print(f"Error processing Rating {rating}: {e}")
        return pd.DataFrame()  # Return an empty DataFrame on error


# Function to run models in parallel
def run_models_in_parallel(data, numerical_col, categorical_col, flagged_data):
    unique_ratings = data['Rating'].unique()
    outliers = pd.DataFrame()

    # Use ThreadPoolExecutor for parallel processing
    with ThreadPoolExecutor() as executor:
        # Submit tasks for each rating
        futures = {
            executor.submit(process_rating, rating, data, numerical_col, categorical_col): rating
            for rating in unique_ratings
        }
        
        # Collect results
        for future in tqdm(futures, desc="Processing Ratings"):
            try:
                result = future.result()
                outliers = pd.concat([outliers, result], ignore_index=True)
            except Exception as e:
                print(f"Error processing rating {futures[future]}: {e}")

    # Combine outliers with flagged data
    final_result = pd.concat([outliers, flagged_data]).reset_index(drop=True)
    return final_result


# Example Usage
if __name__ == "__main__":
    # Assume preprocess_data returns non_flagged_data, flagged_data, categorical_col, numerical_col
    non_flagged_data, flagged_data, categorical_col, numerical_col = preprocess_data("path_to_your_dataset.csv")
    
    # Run the model in parallel
    final_result = run_models_in_parallel(non_flagged_data, numerical_col, categorical_col, flagged_data)
    
    # Save results to a file
    final_result.to_csv("final_results.csv", index=False)
    print("Processing complete. Results saved to 'final_results.csv'.")
