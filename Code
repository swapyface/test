#visuallization 

import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from statsmodels.stats.outliers_influence import variance_inflation_factor
import seaborn as sns
import matplotlib.pyplot as plt

# Load your dataset
# Example: df = pd.read_csv('your_dataset.csv')
# Replace 'your_dataset.csv' with the actual path to your dataset
df = pd.read_csv('your_dataset.csv')

# Define the dependent variable (Y) and independent variables (X)
Y = df['dependent_variable']  # Replace 'dependent_variable' with your actual dependent variable name
X = df.drop('dependent_variable', axis=1)  # Dropping the dependent variable to keep only the independent variables

# Step 1: Correlation with the dependent variable
correlations = X.corrwith(Y)
print("Correlation with the dependent variable:")
print(correlations)

# Visualizing the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()


# Step 2: Checking for multicollinearity using VIF
# Adding a constant column for VIF calculation
X_with_constant = sm.add_constant(X)

# Calculating VIF for each independent variable
vif = pd.DataFrame()
vif["Variable"] = X_with_constant.columns
vif["VIF"] = [variance_inflation_factor(X_with_constant.values, i) for i in range(X_with_constant.shape[1])]
print("\nVariance Inflation Factor (VIF):")
print(vif)

# Step 3: Fit a linear regression model
# Splitting the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Fitting the model
model = LinearRegression()
model.fit(X_train, Y_train)
# Step 4: Display the coefficients and model summary
X_train_with_constant = sm.add_constant(X_train)
ols_model = sm.OLS(Y_train, X_train_with_constant).fit()
print("\nLinear Regression Model Summary:")
print(ols_model.summary())
-------------------------------------------------------------------------
correlations = df.corr()
dependent_var_corr = correlations['dependent_variable'].drop('dependent_variable')
print("Correlation of independent variables with the dependent variable:")
print(dependent_var_corr)

# Visualizing the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlations, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

# Step 2: Checking for multicollinearity using VIF
# Adding a constant column for VIF calculation
X_with_constant = sm.add_constant(X)

# Calculating VIF for each independent variable
vif = pd.DataFrame()
vif["Variable"] = X_with_constant.columns
vif["VIF"] = [variance_inflation_factor(X_with_constant.values, i) for i in range(X_with_constant.shape[1])]
print("\nVariance Inflation Factor (VIF):")
print(vif)


###############################################################


import pandas as pd
from sklearn.preprocessing import LabelEncoder
from scipy.stats import pointbiserialr, f_oneway

# Load your dataset
df = pd.read_csv('your_dataset.csv')

# Identify the categorical and numerical variables
categorical_vars = ['categorical_variable1', 'categorical_variable2']  # Replace with actual categorical variable names
numerical_vars = ['numerical_variable']  # Replace with the actual numerical variable name

# Step 1: Label Encoding for categorical variables (if necessary)
le = LabelEncoder()
for col in categorical_vars:
    df[col + '_encoded'] = le.fit_transform(df[col])

# Step 2: Point Biserial Correlation (for binary categorical variables)
for col in categorical_vars:
    if df[col].nunique() == 2:  # Check if the variable is binary
        corr, p_value = pointbiserialr(df[col + '_encoded'], df[numerical_vars[0]])
        print(f'Point Biserial Correlation between {col} and {numerical_vars[0]}: {corr:.2f}, p-value: {p_value:.2f}')

# Step 3: ANOVA (for categorical variables with more than two levels)
for col in categorical_vars:
    if df[col].nunique() > 2:  # Check if the variable has more than two levels
        groups = [df[numerical_vars[0]][df[col] == level] for level in df[col].unique()]
        f_stat, p_value = f_oneway(*groups)
        print(f'ANOVA between {col} and {numerical_vars[0]}: F-statistic: {f_stat:.2f}, p-value: {p_value:.2f}')
____________________________________________________________________________________----------------------------------------------------

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Sample dataset
data = {
    'Category': ['A', 'B', 'A', 'C', 'B', 'C', 'A'],
    'NumericFeature1': [10, 15, 10, 20, 15, 20, 10],
    'NumericFeature2': [1.5, 2.3, 1.7, 2.9, 2.2, 2.9, 1.6],
    'Target': [100, 150, 120, 200, 160, 210, 110]
}

df = pd.DataFrame(data)

# Features and target
X = df[['Category', 'NumericFeature1', 'NumericFeature2']]
y = df['Target']

# OneHotEncoder for categorical features
categorical_features = ['Category']
numeric_features = ['NumericFeature1', 'NumericFeature2']

# Preprocessing (OneHotEncoding for categorical variables)
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), categorical_features)
    ],
    remainder='passthrough'  # Keeps the numeric features as they are
)

# Building a pipeline
model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Training the model
model_pipeline.fit(X_train, y_train)

# Predicting
y_pred = model_pipeline.predict(X_test)

# Evaluating the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# Output the coefficients
print("Model coefficients:", model_pipeline.named_steps['regressor'].coef_)
print("Model intercept:", model_pipeline.named_steps['regressor'].intercept_)
#######################################################################################


date handling

import pandas as pd
from datetime import datetime

# Assuming df is your DataFrame and 'maturity_date' is the column with maturity dates
df['maturity_date'] = pd.to_datetime(df['maturity_date'])
df['current_date'] = datetime.now()
df['time_to_maturity_days'] = (df['maturity_date'] - df['current_date']).dt.days

2nd mothod


def categorize_maturity(row):
    if row <= 365:
        return 'Short-Term'
    elif 365 < row <= 1825:
        return 'Medium-Term'
    else:
        return 'Long-Term'

df['maturity_category'] = df['time_to_maturity_days'].apply(categorize_maturity)
df = pd.get_dummies(df, columns=['maturity_category'], drop_first=True)


#################################################################################


DB Scan

import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.cluster import DBSCAN
import numpy as np

# Load your dataset
# Replace 'your_dataset.csv' with the path to your dataset file
df = pd.read_csv('your_dataset.csv')

# Preprocessing
# 1. Separate categorical and numerical columns
categorical_cols = df.select_dtypes(include=['object', 'category']).columns
numerical_cols = df.select_dtypes(include=[np.number]).columns

# 2. One-hot encode categorical variables
encoder = OneHotEncoder(sparse=False, drop='first')
encoded_categorical = encoder.fit_transform(df[categorical_cols])

# 3. Combine the encoded categorical data with numerical data
encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_cols))
processed_df = pd.concat([df[numerical_cols].reset_index(drop=True), encoded_categorical_df.reset_index(drop=True)], axis=1)

# 4. Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(processed_df)

# DBSCAN model
# Choose your DBSCAN parameters
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan.fit(scaled_data)

# Identifying anomalies
# In DBSCAN, anomalies are labeled as -1
df['Anomaly'] = dbscan.labels_

# Review the results
anomalies = df[df['Anomaly'] == -1]
print(f"Number of anomalies detected: {len(anomalies)}")
print(anomalies.head())

# Optional: Save anomalies to a CSV file
anomalies.to_csv('detected_anomalies.csv', index=False)

--------------------------------------------------------------------------


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats

# Sample DataFrame (replace with your actual DataFrame)
df = pd.DataFrame({
    'column1': np.random.normal(0, 1, 1000),  # Example of normally distributed data
    'column2': np.random.exponential(1, 1000)  # Example of non-normally distributed data
})

# Function to check normality for a specific column in the DataFrame
def check_normality(df, column):
    data = df[column]
    
    # 1. Visual Inspection using Histogram and Q-Q Plot
    def plot_histogram_and_qq(data, column):
        plt.figure(figsize=(12, 5))

        plt.subplot(1, 2, 1)
        plt.hist(data, bins=30, edgecolor='black')
        plt.title(f'Histogram of {column}')

        plt.subplot(1, 2, 2)
        stats.probplot(data, dist="norm", plot=plt)
        plt.title(f'Q-Q Plot of {column}')

        plt.show()

    # 2. Shapiro-Wilk Test for Normality
    def shapiro_wilk_test(data):
        stat, p_value = stats.shapiro(data)
        print('Shapiro-Wilk Test:')
        print(f'Statistic: {stat:.4f}, p-value: {p_value:.4f}')
        if p_value > 0.05:
            print(f"{column} is likely normally distributed (parametric).")
        else:
            print(f"{column} is likely not normally distributed (non-parametric).")
        print()

    # 3. Kolmogorov-Smirnov Test for Normality
    def kolmogorov_smirnov_test(data):
        d_stat, p_value = stats.kstest(data, 'norm', args=(np.mean(data), np.std(data)))
        print('Kolmogorov-Smirnov Test:')
        print(f'D-statistic: {d_stat:.4f}, p-value: {p_value:.4f}')
        if p_value > 0.05:
            print(f"{column} is likely normally distributed (parametric).")
        else:
            print(f"{column} is likely not normally distributed (non-parametric).")
        print()

    # Plot the Histogram and Q-Q plot
    plot_histogram_and_qq(data, column)

    # Perform Shapiro-Wilk Test
    shapiro_wilk_test(data)

    # Perform Kolmogorov-Smirnov Test
    kolmogorov_smirnov_test(data)

# Example: Check normality for a specific column in the DataFrame
check_normality(df, 'column1')
check_normality(df, 'column2')


