 import pandas as pd

# Assuming your DataFrame is named 'df' and the columns are 'rating' (categorical) and 'dependent_variable'

# Calculate Q1, Q3, and IQR for each category
Q1 = df.groupby('rating')['dependent_variable'].transform(lambda x: x.quantile(0.25))
Q3 = df.groupby('rating')['dependent_variable'].transform(lambda x: x.quantile(0.75))
IQR = Q3 - Q1

# Calculate the lower and upper bounds for each row
Lower_Bound = Q1 - 1.5 * IQR
Upper_Bound = Q3 + 1.5 * IQR

# Filter the DataFrame to keep only values within the IQR bounds for each category
df_filtered = df[(df['dependent_variable'] >= Lower_Bound) & (df['dependent_variable'] <= Upper_Bound)]

# Display the filtered DataFrame with all categories preserved
print(df_filtered)

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.preprocessing import StandardScaler

# Sample data with trade_id
data = {
    'trade_id': [1, 1, 1, 1, 2, 2, 2, 2],
    'price': [10, 20, 15, 1000, 300, 500, 450, 7000],  # Contains anomalies
    'volume': [100, 110, 105, 120, 150, 145, 155, 170],
    'volatility': [0.01, 0.02, 0.015, 0.1, 0.03, 0.04, 0.035, 0.05]
}
df = pd.DataFrame(data)

# Function to train autoencoder and detect anomalies for each trade_id
def detect_anomalies_autoencoder(df_group):
    # Normalize the data
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df_group[['price', 'volume', 'volatility']])

    # Define the autoencoder model
    input_dim = df_scaled.shape[1]
    encoding_dim = 2  # Compression to 2 dimensions

    input_layer = layers.Input(shape=(input_dim,))
    encoded = layers.Dense(encoding_dim, activation='relu')(input_layer)
    decoded = layers.Dense(input_dim, activation='linear')(encoded)

    autoencoder = models.Model(inputs=input_layer, outputs=decoded)

    # Compile the model
    autoencoder.compile(optimizer='adam', loss='mse')

    # Train the autoencoder
    autoencoder.fit(df_scaled, df_scaled, epochs=50, batch_size=2, shuffle=True, validation_split=0.2, verbose=0)

    # Predict using the autoencoder
    df_reconstructed = autoencoder.predict(df_scaled)

    # Calculate reconstruction error
    reconstruction_error = np.mean(np.abs(df_scaled - df_reconstructed), axis=1)

    # Define threshold (can be tuned)
    threshold = np.percentile(reconstruction_error, 95)

    # Identify anomalies
    df_group['reconstruction_error'] = reconstruction_error
    df_group['anomaly'] = df_group['reconstruction_error'] > threshold

    # Inverse transform the scaled data to get original feature values for reconstructed data
    df_reconstructed_orig = scaler.inverse_transform(df_reconstructed)

    # Add the predicted (reconstructed) values to the dataframe
    df_group['predicted_price'] = df_reconstructed_orig[:, 0]
    df_group['predicted_volume'] = df_reconstructed_orig[:, 1]
    df_group['predicted_volatility'] = df_reconstructed_orig[:, 2]

    return df_group

# Group by trade_id and apply the function to each group
df_anomalies = df.groupby('trade_id').apply(detect_anomalies_autoencoder)

# Display anomalies with predicted values
anomalies = df_anomalies[df_anomalies['anomaly'] == True]
print(anomalies[['trade_id', 'price', 'predicted_price', 'volume', 'predicted_volume', 'volatility', 'predicted_volatility', 'reconstruction_error']])
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# Sample data with categorical and numerical variables
data = {
    'trade_id': [1, 1, 1, 1, 2, 2, 2, 2],
    'category': ['A', 'A', 'B', 'B', 'A', 'B', 'B', 'A'],  # Categorical variable
    'price': [10, 20, 15, 1000, 300, 500, 450, 7000],  # Contains anomalies
    'volume': [100, 110, 105, 120, 150, 145, 155, 170],
    'volatility': [0.01, 0.02, 0.015, 0.1, 0.03, 0.04, 0.035, 0.05]
}
df = pd.DataFrame(data)

# Function to train autoencoder and detect anomalies for each trade_id
def detect_anomalies_autoencoder(df_group):
    # Define numerical and categorical columns
    numerical_cols = ['price', 'volume', 'volatility']
    categorical_cols = ['category']

    # Preprocess numerical and categorical data
    column_transformer = ColumnTransformer([
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(), categorical_cols)
    ])

    # Transform the data
    df_transformed = column_transformer.fit_transform(df_group)

    # Define the autoencoder model
    input_dim = df_transformed.shape[1]
    encoding_dim = 5  # Adjust based on your data

    input_layer = layers.Input(shape=(input_dim,))
    encoded = layers.Dense(encoding_dim, activation='relu')(input_layer)
    decoded = layers.Dense(input_dim, activation='linear')(encoded)

    autoencoder = models.Model(inputs=input_layer, outputs=decoded)

    # Compile the model
    autoencoder.compile(optimizer='adam', loss='mse')

    # Train the autoencoder
    autoencoder.fit(df_transformed, df_transformed, epochs=50, batch_size=2, shuffle=True, validation_split=0.2, verbose=0)

    # Predict using the autoencoder
    df_reconstructed = autoencoder.predict(df_transformed)

    # Calculate reconstruction error
    reconstruction_error = np.mean(np.abs(df_transformed - df_reconstructed), axis=1)

    # Define threshold (can be tuned)
    threshold = np.percentile(reconstruction_error, 95)

    # Identify anomalies
    df_group['reconstruction_error'] = reconstruction_error
    df_group['anomaly'] = df_group['reconstruction_error'] > threshold

    # Inverse transform the numerical data for reconstructed values
    df_numerical_reconstructed = column_transformer.named_transformers_['num'].inverse_transform(df_reconstructed[:, :len(numerical_cols)])

    # Add the predicted (reconstructed) values to the dataframe
    df_group['predicted_price'] = df_numerical_reconstructed[:, 0]
    df_group['predicted_volume'] = df_numerical_reconstructed[:, 1]
    df_group['predicted_volatility'] = df_numerical_reconstructed[:, 2]

    return df_group

# Group by trade_id and apply the function to each group
df_anomalies = df.groupby('trade_id').apply(detect_anomalies_autoencoder)

# Display anomalies with predicted values
anomalies = df_anomalies[df_anomalies['anomaly'] == True]
print(anomalies[['trade_id', 'category', 'price', 'predicted_price', 'volume', 'predicted_volume', 'volatility', 'predicted_volatility', 'reconstruction_error']])