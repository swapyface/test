from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.ensemble import IsolationForest
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
import numpy as np
from typing import Dict, Any


class DataEncoder:
    def __init__(self, numerical_cols: list[str], categorical_cols: list[str]):
        self.numerical_cols = numerical_cols
        self.categorical_cols = categorical_cols
        self.scaler = MinMaxScaler()
        self.encoder = OneHotEncoder(sparse_output=False, drop="first")

    def encode(self, data: pd.DataFrame) -> np.ndarray:
        """Encodes the numerical and categorical columns in the dataset."""
        # Scale numerical data
        scaled_data = self.scaler.fit_transform(data[self.numerical_cols])

        # Encode categorical data
        encoded_categorical = self.encoder.fit_transform(data[self.categorical_cols])

        # Combine encoded data
        return np.hstack((encoded_categorical, scaled_data))


class IsolationForestConfig:
    @staticmethod
    def get_parameters(dataset_size: int) -> Dict[str, Any]:
        """Returns Isolation Forest parameters based on dataset size."""
        if dataset_size < 100:
            return {"n_estimators": 20, "max_samples": dataset_size, "contamination": 0.05, "max_features": 1.0}
        elif 100 <= dataset_size < 500:
            return {"n_estimators": 50, "max_samples": 100, "contamination": 0.05, "max_features": 1.0}
        elif 500 <= dataset_size < 1000:
            return {"n_estimators": 100, "max_samples": 256, "contamination": 0.05, "max_features": 1.0}
        elif 1000 <= dataset_size < 5000:
            return {"n_estimators": 300, "max_samples": 500, "contamination": 0.05, "max_features": 1.0}
        elif 5000 <= dataset_size < 10000:
            return {"n_estimators": 500, "max_samples": 1500, "contamination": 0.05, "max_features": 1.0}
        elif 10000 <= dataset_size < 50000:
            return {"n_estimators": 1000, "max_samples": 3000, "contamination": 0.05, "max_features": 1.0}
        elif 50000 <= dataset_size < 100000:
            return {"n_estimators": 1500, "max_samples": 5000, "contamination": 0.05, "max_features": 1.0}
        elif 100000 <= dataset_size < 300000:
            return {"n_estimators": 2000, "max_samples": 6000, "contamination": 0.05, "max_features": 1.0}
        elif 300000 <= dataset_size < 500000:
            return {"n_estimators": 5000, "max_samples": 15000, "contamination": 0.05, "max_features": 1.0}
        elif 500000 <= dataset_size < 1000000:
            return {"n_estimators": 7000, "max_samples": 20000, "contamination": 0.05, "max_features": 1.0}
        else:
            raise ValueError("Dataset size out of supported range.")


class RatingProcessor:
    def __init__(self, numerical_cols: list[str], categorical_cols: list[str]):
        self.encoder = DataEncoder(numerical_cols, categorical_cols)

    def process_rating(self, data: pd.DataFrame, rating: int) -> pd.DataFrame:
        """Processes data for a specific rating."""
        try:
            print(f"Processing Rating: {rating}")

            # Filter data for the specific rating
            rating_data = data[data["Rating"] == rating]

            # Get Isolation Forest parameters
            params = IsolationForestConfig.get_parameters(len(rating_data))
            print(f"Rating {rating} parameters: {params}")

            # Initialize Isolation Forest model
            iso_model = IsolationForest(
                n_estimators=params["n_estimators"],
                max_samples=params["max_samples"],
                contamination=params["contamination"],
                max_features=params["max_features"],
                random_state=42,
            )

            # Encode data
            X = self.encoder.encode(rating_data)

            # Predict outliers
            rating_data["ISO_result"] = iso_model.fit_predict(X)

            # Replace -1 and 1 with "outlier" and "inlier"
            rating_data["ISO_result"] = rating_data["ISO_result"].replace({-1: "outlier", 1: "inlier"})

            print(rating_data.head())
            return rating_data

        except Exception as e:
            print(f"Error processing Rating {rating}: {e}")
            return pd.DataFrame()


class ParallelModelRunner:
    def __init__(self, numerical_cols: list[str], categorical_cols: list[str]):
        self.processor = RatingProcessor(numerical_cols, categorical_cols)

    def run(self, data: pd.DataFrame, flagged_data: pd.DataFrame) -> pd.DataFrame:
        """Runs models in parallel for all ratings."""
        unique_ratings = data["Rating"].unique()
        outliers = pd.DataFrame()

        # Use ThreadPoolExecutor for parallel processing
        with ThreadPoolExecutor() as executor:
            futures = {
                executor.submit(self.processor.process_rating, data, rating): rating
                for rating in unique_ratings
            }

            for future in as_completed(futures):
                try:
                    result = future.result()
                    outliers = pd.concat([outliers, result], ignore_index=True)
                except Exception as e:
                    print(f"Error processing rating {futures[future]}: {e}")

        # Combine outliers with flagged data
        final_result = pd.concat([outliers, flagged_data]).reset_index(drop=True)
        return final_result
