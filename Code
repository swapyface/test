#visuallization 

import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from statsmodels.stats.outliers_influence import variance_inflation_factor
import seaborn as sns
import matplotlib.pyplot as plt

# Load your dataset
# Example: df = pd.read_csv('your_dataset.csv')
# Replace 'your_dataset.csv' with the actual path to your dataset
df = pd.read_csv('your_dataset.csv')

# Define the dependent variable (Y) and independent variables (X)
Y = df['dependent_variable']  # Replace 'dependent_variable' with your actual dependent variable name
X = df.drop('dependent_variable', axis=1)  # Dropping the dependent variable to keep only the independent variables

# Step 1: Correlation with the dependent variable
correlations = X.corrwith(Y)
print("Correlation with the dependent variable:")
print(correlations)

# Visualizing the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()


# Step 2: Checking for multicollinearity using VIF
# Adding a constant column for VIF calculation
X_with_constant = sm.add_constant(X)

# Calculating VIF for each independent variable
vif = pd.DataFrame()
vif["Variable"] = X_with_constant.columns
vif["VIF"] = [variance_inflation_factor(X_with_constant.values, i) for i in range(X_with_constant.shape[1])]
print("\nVariance Inflation Factor (VIF):")
print(vif)

# Step 3: Fit a linear regression model
# Splitting the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Fitting the model
model = LinearRegression()
model.fit(X_train, Y_train)
# Step 4: Display the coefficients and model summary
X_train_with_constant = sm.add_constant(X_train)
ols_model = sm.OLS(Y_train, X_train_with_constant).fit()
print("\nLinear Regression Model Summary:")
print(ols_model.summary())
-------------------------------------------------------------------------
correlations = df.corr()
dependent_var_corr = correlations['dependent_variable'].drop('dependent_variable')
print("Correlation of independent variables with the dependent variable:")
print(dependent_var_corr)

# Visualizing the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlations, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

# Step 2: Checking for multicollinearity using VIF
# Adding a constant column for VIF calculation
X_with_constant = sm.add_constant(X)

# Calculating VIF for each independent variable
vif = pd.DataFrame()
vif["Variable"] = X_with_constant.columns
vif["VIF"] = [variance_inflation_factor(X_with_constant.values, i) for i in range(X_with_constant.shape[1])]
print("\nVariance Inflation Factor (VIF):")
print(vif)


###############################################################


import pandas as pd
from sklearn.preprocessing import LabelEncoder
from scipy.stats import pointbiserialr, f_oneway

# Load your dataset
df = pd.read_csv('your_dataset.csv')

# Identify the categorical and numerical variables
categorical_vars = ['categorical_variable1', 'categorical_variable2']  # Replace with actual categorical variable names
numerical_vars = ['numerical_variable']  # Replace with the actual numerical variable name

# Step 1: Label Encoding for categorical variables (if necessary)
le = LabelEncoder()
for col in categorical_vars:
    df[col + '_encoded'] = le.fit_transform(df[col])

# Step 2: Point Biserial Correlation (for binary categorical variables)
for col in categorical_vars:
    if df[col].nunique() == 2:  # Check if the variable is binary
        corr, p_value = pointbiserialr(df[col + '_encoded'], df[numerical_vars[0]])
        print(f'Point Biserial Correlation between {col} and {numerical_vars[0]}: {corr:.2f}, p-value: {p_value:.2f}')

# Step 3: ANOVA (for categorical variables with more than two levels)
for col in categorical_vars:
    if df[col].nunique() > 2:  # Check if the variable has more than two levels
        groups = [df[numerical_vars[0]][df[col] == level] for level in df[col].unique()]
        f_stat, p_value = f_oneway(*groups)
        print(f'ANOVA between {col} and {numerical_vars[0]}: F-statistic: {f_stat:.2f}, p-value: {p_value:.2f}')
____________________________________________________________________________________----------------------------------------------------

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Sample dataset
data = {
    'Category': ['A', 'B', 'A', 'C', 'B', 'C', 'A'],
    'NumericFeature1': [10, 15, 10, 20, 15, 20, 10],
    'NumericFeature2': [1.5, 2.3, 1.7, 2.9, 2.2, 2.9, 1.6],
    'Target': [100, 150, 120, 200, 160, 210, 110]
}

df = pd.DataFrame(data)

# Features and target
X = df[['Category', 'NumericFeature1', 'NumericFeature2']]
y = df['Target']

# OneHotEncoder for categorical features
categorical_features = ['Category']
numeric_features = ['NumericFeature1', 'NumericFeature2']

# Preprocessing (OneHotEncoding for categorical variables)
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), categorical_features)
    ],
    remainder='passthrough'  # Keeps the numeric features as they are
)

# Building a pipeline
model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Training the model
model_pipeline.fit(X_train, y_train)

# Predicting
y_pred = model_pipeline.predict(X_test)

# Evaluating the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# Output the coefficients
print("Model coefficients:", model_pipeline.named_steps['regressor'].coef_)
print("Model intercept:", model_pipeline.named_steps['regressor'].intercept_)
#######################################################################################


date handling

import pandas as pd
from datetime import datetime

# Assuming df is your DataFrame and 'maturity_date' is the column with maturity dates
df['maturity_date'] = pd.to_datetime(df['maturity_date'])
df['current_date'] = datetime.now()
df['time_to_maturity_days'] = (df['maturity_date'] - df['current_date']).dt.days

2nd mothod


def categorize_maturity(row):
    if row <= 365:
        return 'Short-Term'
    elif 365 < row <= 1825:
        return 'Medium-Term'
    else:
        return 'Long-Term'

df['maturity_category'] = df['time_to_maturity_days'].apply(categorize_maturity)
df = pd.get_dummies(df, columns=['maturity_category'], drop_first=True)


#################################################################################


DB Scan

import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.cluster import DBSCAN
import numpy as np

# Load your dataset
# Replace 'your_dataset.csv' with the path to your dataset file
df = pd.read_csv('your_dataset.csv')

# Preprocessing
# 1. Separate categorical and numerical columns
categorical_cols = df.select_dtypes(include=['object', 'category']).columns
numerical_cols = df.select_dtypes(include=[np.number]).columns

# 2. One-hot encode categorical variables
encoder = OneHotEncoder(sparse=False, drop='first')
encoded_categorical = encoder.fit_transform(df[categorical_cols])

# 3. Combine the encoded categorical data with numerical data
encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_cols))
processed_df = pd.concat([df[numerical_cols].reset_index(drop=True), encoded_categorical_df.reset_index(drop=True)], axis=1)

# 4. Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(processed_df)

# DBSCAN model
# Choose your DBSCAN parameters
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan.fit(scaled_data)

# Identifying anomalies
# In DBSCAN, anomalies are labeled as -1
df['Anomaly'] = dbscan.labels_

# Review the results
anomalies = df[df['Anomaly'] == -1]
print(f"Number of anomalies detected: {len(anomalies)}")
print(anomalies.head())

# Optional: Save anomalies to a CSV file
anomalies.to_csv('detected_anomalies.csv', index=False)

--------------------------------------------------------------------------


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats

# Sample DataFrame (replace with your actual DataFrame)
df = pd.DataFrame({
    'column1': np.random.normal(0, 1, 1000),  # Example of normally distributed data
    'column2': np.random.exponential(1, 1000)  # Example of non-normally distributed data
})

# Function to check normality for a specific column in the DataFrame
def check_normality(df, column):
    data = df[column]
    
    # 1. Visual Inspection using Histogram and Q-Q Plot
    def plot_histogram_and_qq(data, column):
        plt.figure(figsize=(12, 5))

        plt.subplot(1, 2, 1)
        plt.hist(data, bins=30, edgecolor='black')
        plt.title(f'Histogram of {column}')

        plt.subplot(1, 2, 2)
        stats.probplot(data, dist="norm", plot=plt)
        plt.title(f'Q-Q Plot of {column}')

        plt.show()

    # 2. Shapiro-Wilk Test for Normality
    def shapiro_wilk_test(data):
        stat, p_value = stats.shapiro(data)
        print('Shapiro-Wilk Test:')
        print(f'Statistic: {stat:.4f}, p-value: {p_value:.4f}')
        if p_value > 0.05:
            print(f"{column} is likely normally distributed (parametric).")
        else:
            print(f"{column} is likely not normally distributed (non-parametric).")
        print()

    # 3. Kolmogorov-Smirnov Test for Normality
    def kolmogorov_smirnov_test(data):
        d_stat, p_value = stats.kstest(data, 'norm', args=(np.mean(data), np.std(data)))
        print('Kolmogorov-Smirnov Test:')
        print(f'D-statistic: {d_stat:.4f}, p-value: {p_value:.4f}')
        if p_value > 0.05:
            print(f"{column} is likely normally distributed (parametric).")
        else:
            print(f"{column} is likely not normally distributed (non-parametric).")
        print()

    # Plot the Histogram and Q-Q plot
    plot_histogram_and_qq(data, column)

    # Perform Shapiro-Wilk Test
    shapiro_wilk_test(data)

    # Perform Kolmogorov-Smirnov Test
    kolmogorov_smirnov_test(data)

# Example: Check normality for a specific column in the DataFrame
check_normality(df, 'column1')
check_normality(df, 'column2')


_________________________________________________________________

outliear dectection 

result = df.groupby('Category')['Dependent_Variable'].agg(['min', 'max']).reset_index()


import pandas as pd

# Sample data
data = {
    'r': ['A', 'B', 'A', 'B', 'C', 'A', 'C', 'B', 'C', 'A'],
    'Dependent_Variable': [5.2, 10.5, 15.8, 7.3, 12.1, 3.6, 8.9, 6.7, 14.5, 20.2]
}

# Create DataFrame
df = pd.DataFrame(data)

# Function to detect outliers using the IQR method
def find_outliers(group):
    Q1 = group.quantile(0.25)
    Q3 = group.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return group[(group < lower_bound) | (group > upper_bound)]

# Apply the function to each category in the 'Rating' column
outliers = df.groupby('Rating')['Dependent_Variable'].apply(find_outliers).reset_index()

# Display the outliers
print(outliers)
------
import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.cluster import DBSCAN
import numpy as np

# Sample data with categorical, numerical, and extra columns
data = {
    'category': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B'],
    'numerical': [10, 15, 14, 10, 23, 20, 30, 25],
    'extra_column1': ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8'],
    'extra_column2': [100, 200, 150, 120, 300, 250, 280, 240]
}

# Create DataFrame
df = pd.DataFrame(data)

# Step 1: Keep the columns you don't want to pass to DBSCAN
extra_columns = df[['extra_column1', 'extra_column2']]

# Step 2: Preprocess data (excluding the extra columns)
# One-Hot Encoding for the categorical variable
encoder = OneHotEncoder(sparse=False)
encoded_cat = encoder.fit_transform(df[['category']])

# Standardize the numerical column
scaler = StandardScaler()
scaled_num = scaler.fit_transform(df[['numerical']])

# Combine encoded categorical and scaled numerical data
X = np.hstack((encoded_cat, scaled_num))

# Step 3: Apply DBSCAN (without passing the extra columns)
dbscan = DBSCAN(eps=1.5, min_samples=2)
labels = dbscan.fit_predict(X)

# Step 4: Add the cluster labels back to the original DataFrame
df['cluster'] = labels

# Step 5: Identify anomalies (outliers have a cluster label of -1)
df['is_anomaly'] = df['cluster'] == -1

# Step 6: Join the extra columns back to the DataFrame
df_with_extra = pd.concat([df[['category', 'numerical', 'cluster', 'is_anomaly']], extra_columns], axis=1)

# Output the final DataFrame with anomalies detected
print(df_with_extra)


# Step 6: Join the extra columns back to the DataFrame
df_with_extra = pd.concat([df[categorical_cols + numerical_cols + ['cluster', 'is_anomaly']], extra_columns], axis=import pandas as pd
import numpy as np

# Sample dataframe with additional columns
data = {'Category': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],
        'NumericColumn': [10, 15, 30, 25, 40, 45, 100, 110, 150],
        'OtherColumn1': ['X', 'Y', 'Z', 'X', 'Y', 'Z', 'X', 'Y', 'Z'],
        'OtherColumn2': [1, 2, 3, 4, 5, 6, 7, 8, 9]}

df = pd.DataFrame(data)

# Function to filter rows that are outside 2 standard deviations for each group
def find_outliers(group):
    mean = group['NumericColumn'].mean()
    std_dev = group['NumericColumn'].std()
    lower_bound = mean - 2 * std_dev
    upper_bound = mean + 2 * std_dev
    # Return all columns while filtering based on NumericColumn
    return group[(group['NumericColumn'] < lower_bound) | (group['NumericColumn'] > upper_bound)]

# Apply the function to each group
outliers = df.groupby('Category').apply(find_outliers).reset_index(drop=True)

print(outliers)

 Function to retain rows that are within 2 standard deviations for each group
def drop_outliers(group):
    mean = group['NumericColumn'].mean()
    std_dev = group['NumericColumn'].std()
    lower_bound = mean - 2 * std_dev
    upper_bound = mean + 2 * std_dev
    # Retain only rows that are within the 2 standard deviations
    return group[(group['NumericColumn'] >= lower_bound) & (group['NumericColumn'] <= upper_bound)]

# Apply the function to each group to drop outliers
cleaned_df = df.groupby('Category').apply(drop_outliers).reset_index(drop=True)

print(cleaned_df)

Thank you, Sid, for the heads-up. I understand Adithya's importance on the call. However, I'd like to proceed with our scheduled meeting and go over the key points quickly. This will allow me to do further analysis. 

We can also arrange for a follow-up meeting next week, when Adi is available.
Let me know if this works for you. 


