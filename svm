import pandas as pd
import numpy as np
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor

# Set random seed for reproducibility
np.random.seed(42)

# Generate synthetic data with a wide range of values
num_samples = 10_000

face_value = np.random.uniform(-1e6, 1e6, num_samples)  # Large and small values
factor = np.random.uniform(-100, 100, num_samples)      # Include negative and positive factors
price = np.random.uniform(-1e3, 1e3, num_samples)       # Wide range of prices

# Calculate market value
market_value = (face_value * factor * price) / 100

# Create DataFrame
df = pd.DataFrame({
    "face_value": face_value,
    "factor": factor,
    "price": price,
    "market_value": market_value
})


# Define features and target variable
X = df[["face_value", "factor", "price"]]
y = df["market_value"]

# Apply StandardScaler to features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data into training (80%) and testing (20%)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize the baseline XGBoost regressor
xgb_baseline = XGBRegressor(objective="reg:squarederror", random_state=42)

# Train the baseline model
xgb_baseline.fit(X_train, y_train)

# Predict on test data
y_pred_baseline = xgb_baseline.predict(X_test)

# Evaluate baseline model
baseline_rmse = np.sqrt(mean_squared_error(y_test, y_pred_baseline))
baseline_mae = mean_absolute_error(y_test, y_pred_baseline)
baseline_r2 = r2_score(y_test, y_pred_baseline)

print("ðŸš€ Baseline XGBoost Model Performance:")
print(f"RMSE: {baseline_rmse:.4f}")
print(f"MAE: {baseline_mae:.4f}")
print(f"RÂ²: {baseline_r2:.4f}")

# -------------------------------------------

# Define hyperparameter grid
param_grid = {
    "n_estimators": [100, 300, 500],
    "learning_rate": [0.01, 0.05, 0.1],
    "max_depth": [3, 5, 7],
    "min_child_weight": [1, 3, 5],
    "gamma": [0, 1, 3],
    "subsample": [0.7, 1.0],
    "colsample_bytree": [0.7, 1.0],
    "reg_lambda": [0, 1, 5],
    "reg_alpha": [0, 1, 5],
}

# Initialize XGBoost regressor
xgb = XGBRegressor(objective="reg:squarederror", random_state=42)

# Perform GridSearchCV (Warning: This may take a long time)
grid_search = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    cv=3,
    scoring="neg_root_mean_squared_error",
    verbose=2,
    n_jobs=-1
)

# Fit GridSearchCV
grid_search.fit(X_train, y_train)

# Get best parameters
best_params = grid_search.best_params_
print("\nâœ… Best Hyperparameters:", best_params)

# Train final model with best parameters
xgb_tuned = XGBRegressor(objective="reg:squarederror", **best_params, random_state=42)
xgb_tuned.fit(X_train, y_train)

# Predict using tuned model
y_pred_tuned = xgb_tuned.predict(X_test)

# Evaluate tuned model
tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_tuned))
tuned_mae = mean_absolute_error(y_test, y_pred_tuned)
tuned_r2 = r2_score(y_test, y_pred_tuned)

print("\nðŸš€ Fine-Tuned XGBoost Model Performance:")
print(f"RMSE: {tuned_rmse:.4f}")
print(f"MAE: {tuned_mae:.4f}")
print(f"RÂ²: {tuned_r2:.4f}")

