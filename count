import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from scipy.stats import zscore
from sklearn.metrics import confusion_matrix, classification_report

# Example dataset (replace with your own data)
data = pd.DataFrame({
    'price': np.random.normal(100, 20, 300000),  # Example numerical column
    'category_1': np.random.choice(['A', 'B', 'C'], size=300000),  # Example categorical column
    'category_2': np.random.choice(['X', 'Y', 'Z'], size=300000)   # Another categorical column
})

# Preprocessing (one-hot encoding for categorical variables)
data_encoded = pd.get_dummies(data, drop_first=True)

# Isolation Forest Model
model = IsolationForest(contamination=0.05, random_state=42)
model.fit(data_encoded)

# Predicting anomalies (-1 = outlier, 1 = inlier)
predictions = model.predict(data_encoded)

# Extract anomaly scores (lower score means more likely to be an outlier)
anomaly_scores = model.decision_function(data_encoded)

# Step 1: Calculate z-scores for anomaly scores
z_scores = zscore(anomaly_scores)

# Step 2: Define threshold for labeling outliers based on z-scores
# For instance, if |z-score| > 2, we consider it an outlier
z_score_labels = np.where(abs(z_scores) > 2, -1, 1)  # -1 for outliers, 1 for inliers

# Step 3: Create confusion matrix comparing model predictions with z-score labels
cm = confusion_matrix(z_score_labels, predictions, labels=[-1, 1])

# Display the confusion matrix
print("Confusion Matrix:\n", cm)

# Generate a classification report to understand precision, recall, F1-score
report = classification_report(z_score_labels, predictions, target_names=['Outliers', 'Inliers'])
print("\nClassification Report:\n", report)