was responsible for building a scalable, auditable solution to map vendor instruments to internal instruments with very high precision, while also improving coverage and reducing manual stewardship. The solution needed to run at scale (millions of records), integrate into existing data pipelines, and provide traceability suitable for model governance and controls.

Action

I implemented a two-stage mapping pipeline using PySpark for large-scale ETL and feature engineering, and a Graph Neural Network (GNN) for graph-based entity resolution.

1) PySpark ingestion, normalization, and deterministic mapping (baseline)
	•	Built PySpark jobs to ingest daily vendor drops and reference/security master snapshots from the lake (S3/ADLS/HDFS depending on environment).
	•	Standardized instrument attributes (issuer names, security descriptions, dates, coupon formats, currency codes, country codes).
	•	Implemented deterministic joins first using high-trust keys:
	•	ISIN/CUSIP/FIGI where available, plus validations (currency, product type, maturity sanity checks).
	•	This baseline resolved a large chunk of records with near-zero operational risk and provided a clean “unmapped remainder” for ML.

2) Candidate generation at scale (blocking) in PySpark
	•	To avoid O(N²) comparisons, I used PySpark “blocking” to generate candidate pairs only when they were plausible:
	•	Same product type + currency
	•	Maturity within a configurable window (e.g., ±365 days for bonds; tenor alignment for CDS)
	•	Issuer token match / LEI match when present
	•	This reduced candidate pairs from billions to a manageable set (typically tens per instrument), enabling ML scoring to scale.

3) Graph construction for identity resolution
	•	Modeled the problem as an identity graph:
	•	Nodes: VendorInstrument, RefInstrument, Identifier (ISIN/CUSIP/SEDOL/FIGI), and optionally Issuer/Entity
	•	Edges: HAS_ID, ISSUED_BY, plus candidate edges
	•	The key benefit was that the GNN could use graph structure (shared identifiers, issuer relationships, neighborhood consistency) to infer correct matches even when direct keys were missing.

4) GNN link prediction for match scoring
	•	Trained a link prediction model to estimate P(vendor ↔ ref match) on candidate edges:
	•	Node features combined:
	•	Text embeddings of instrument/issuer names (normalized descriptions)
	•	Numeric features (coupon, maturity in days, issue date)
	•	Categorical embeddings (currency, seniority, doc clause, region)
	•	Used a relational/heterogeneous GNN architecture (e.g., R-GCN or HGT) to handle multiple node/edge types.
	•	For training labels, I used:
	•	Existing historical mapping tables and steward-approved links as positives
	•	Hard negatives from “close but wrong” candidates (same issuer + similar maturity but different currency/seniority)

5) Global consistency and safe decisioning
	•	After scoring, I applied:
	•	Hard constraints (currency/product type mismatch blocked)
	•	A confidence thresholding policy:
	•	High score → auto-link
	•	Medium score → steward review queue
	•	Low score → exception/no-link
	•	To prevent duplicates and enforce consistency, I used a global selection step (maximum weight matching / capacity rules) so the system did not map multiple vendor instruments incorrectly to the same internal security unless explicitly allowed.

6) Governance, explainability, and monitoring
	•	Persisted a versioned mapping table with:
	•	match_confidence, evidence flags (shared ID, issuer proximity), and model version
	•	Monitored:
	•	Precision in auto-link bucket
	•	Coverage improvements
	•	Downstream break rate (pricing/risk exceptions)
	•	Data drift (issuer name changes, symbology churn)

Result
	•	Improved mapping coverage substantially while maintaining very high precision in the auto-link tier.
	•	Reduced manual reconciliation load by routing only ambiguous cases to stewards.
	•	Stabilized downstream pricing and risk jobs by ensuring market data joined reliably to internal positions, improving operational resilience and reporting quality.